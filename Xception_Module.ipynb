{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "CxBOuypwaiNj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SeperableConv2d(nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1):\n",
        "    super(SeperableConv2d, self).__init__()\n",
        "\n",
        "    self.depthwise = nn.Conv2d(\n",
        "        in_channels, in_channels,\n",
        "        kernel_size = kernel_size,\n",
        "        stride = stride, padding = padding,\n",
        "        groups = in_channels, bias = False\n",
        "    )\n",
        "\n",
        "\n",
        "    self.pointwise = nn.Conv2d(\n",
        "        in_channels, out_channels,\n",
        "        kernel_size = 1, stride = 1,\n",
        "        padding = 0, bias = False\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.depthwise(x)\n",
        "    out = self.pointwise(out)\n",
        "    return out\n",
        "\n",
        "# We created a seperable convolutional layer which check the image both depthwise like calculate each channel seperately using group = in_channels and get features for each channel\n",
        "# Then it calculate like normal convolution layer by getting spatial features\n",
        "# The inception module will do both spatial and channel wise calculations together but in Xception module it will do seperately\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, main_path, shortcut):\n",
        "        super().__init__()\n",
        "        self.main_path = main_path\n",
        "        self.shortcut = shortcut\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main_path(x) + self.shortcut(x)\n",
        "\n",
        "# The Residual Block will add the two paths through skip connections here"
      ],
      "metadata": {
        "id": "drt4J_KAfz_v"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniXception(nn.Module):\n",
        "\n",
        "  def __init__(self, num_classes = 10):\n",
        "    super(MiniXception, self).__init__()\n",
        "\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3, padding = 1)\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3, padding = 1)\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "\n",
        "    self.block1 = self._make_residual_block(64, 128, stride = 2)\n",
        "    self.block2 = self._make_residual_block(128, 256, stride = 2)\n",
        "    self.block3  = self._make_residual_block(256, 256, stride = 1)\n",
        "\n",
        "    self.sep_final = SeperableConv2d(256, 512, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.bn_final = nn.BatchNorm2d(512)\n",
        "\n",
        "    self.gap = nn.AdaptiveAvgPool2d((1,1))\n",
        "    self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "\n",
        "  def _make_residual_block(self, in_c, out_c, stride):\n",
        "\n",
        "    layers = []\n",
        "    layers.append(nn.ReLU(inplace = True))\n",
        "    layers.append(SeperableConv2d(in_c, out_c, kernel_size = 3, stride = stride, padding = 1))\n",
        "    layers.append(nn.BatchNorm2d(out_c))\n",
        "\n",
        "    layers.append(nn.ReLU(inplace = True))\n",
        "    layers.append(SeperableConv2d(out_c, out_c, kernel_size = 3, stride = 1, padding = 1))\n",
        "    layers.append(nn.BatchNorm2d(out_c))\n",
        "\n",
        "    main_path = nn.Sequential(*layers)\n",
        "\n",
        "    shortcut = nn.Sequential()\n",
        "    if stride != 1:\n",
        "      shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_c, out_c, 1,  stride = stride, bias = False),\n",
        "          nn.BatchNorm2d(out_c)\n",
        "      )\n",
        "\n",
        "    return ResidualBlock(main_path, shortcut)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "      # Entry\n",
        "      x = self.conv1(x)\n",
        "      x = self.bn1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.bn2(x)\n",
        "\n",
        "      # Middle (Residuals)\n",
        "      x = self.block1(x)\n",
        "      x = self.block2(x)\n",
        "      x = self.block3(x)\n",
        "\n",
        "      # Exit\n",
        "      x = self.sep_final(x)\n",
        "      x = self.bn_final(x)\n",
        "      x = self.relu(x)\n",
        "\n",
        "      x = self.gap(x)\n",
        "      x = x.view(x.size(0), -1)\n",
        "      x = self.fc(x)\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "2DLObYuPrclb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7t6ncGVoeL0",
        "outputId": "a50c4bff-d1f3-4d8a-d60e-b711c9c50b4f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MiniXception().to(device)"
      ],
      "metadata": {
        "id": "LXEq_z25oke5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomCrop(32, padding = 4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.Normalize(mean = (0.5,0.5,0.5), std = (0.5,0.5,0.5))\n",
        "])\n",
        "# we used random rotation, horizontal flip and crop to make the model to identify inconsistencies by augmenting new data\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "f5fhvEa2o9Ma"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root = 'data', train = True, download = True, transform = train_transform)\n",
        "test_dataset = datasets.CIFAR10(root = \"data\", train = False, download = True, transform = test_transform)"
      ],
      "metadata": {
        "id": "WChqEGT41pMX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PIN_MEMORY = True if torch.cuda.is_available() else False\n",
        "NUM_WORKERS = 2\n",
        "BATCH_SIZE = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle = True,\n",
        "                          pin_memory = PIN_MEMORY, num_workers = NUM_WORKERS, drop_last = True, persistent_workers= True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle = False,\n",
        "                         pin_memory = PIN_MEMORY, num_workers = NUM_WORKERS, drop_last = False)"
      ],
      "metadata": {
        "id": "uE9MMNzV2lVU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr = 0.001, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "qF0M7J8b4mQn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "  model.train()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "\n",
        "  for X,y in dataloader:\n",
        "    X,y = X.to(device), y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    _, predicted = torch.max(pred,1)\n",
        "    total += y.size(0)\n",
        "    correct += (predicted == y).sum().item()\n",
        "\n",
        "  return running_loss/len(dataloader), correct/total\n"
      ],
      "metadata": {
        "id": "bnux48zG7z_4"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(dataloader, model, loss_fn):\n",
        "\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X,y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "      running_loss += loss.item()\n",
        "      _, predicted = torch.max(pred, 1)\n",
        "      total += y.size(0)\n",
        "      correct += (predicted == y).sum().item()\n",
        "\n",
        "  return running_loss/len(dataloader), correct/total"
      ],
      "metadata": {
        "id": "bM9AcKxq9jzz"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 25\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  train_loss, train_acc = train_one_epoch(train_loader, model, loss_fn, optimizer)\n",
        "  test_loss, test_acc = evaluate(test_loader, model, loss_fn)\n",
        "\n",
        "  print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
        "          f\"Train: {train_loss:.4f} ({train_acc:.2f}%) | \"\n",
        "          f\"Test: {test_loss:.4f} ({test_acc:.2f}%)\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"Total time: {total_time:.2f} seconds\")\n",
        "\n",
        "# the model stopped learning after 15th epoch we should use learning scheduler to change the learning pace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1qxEriM-JtZ",
        "outputId": "ebdd115e-6cd1-4f25-d825-478e9152d05d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25 | Train: 1.3784 (0.49%) | Test: 1.3770 (0.56%)\n",
            "Epoch 2/25 | Train: 0.9623 (0.66%) | Test: 0.9174 (0.68%)\n",
            "Epoch 3/25 | Train: 0.7904 (0.72%) | Test: 0.7687 (0.74%)\n",
            "Epoch 4/25 | Train: 0.6853 (0.76%) | Test: 0.6569 (0.77%)\n",
            "Epoch 5/25 | Train: 0.6208 (0.78%) | Test: 0.6027 (0.79%)\n",
            "Epoch 6/25 | Train: 0.5752 (0.80%) | Test: 0.6204 (0.79%)\n",
            "Epoch 7/25 | Train: 0.5389 (0.81%) | Test: 0.5565 (0.81%)\n",
            "Epoch 8/25 | Train: 0.5011 (0.83%) | Test: 0.5370 (0.83%)\n",
            "Epoch 9/25 | Train: 0.4768 (0.83%) | Test: 0.5463 (0.82%)\n",
            "Epoch 10/25 | Train: 0.4481 (0.84%) | Test: 0.4938 (0.83%)\n",
            "Epoch 11/25 | Train: 0.4301 (0.85%) | Test: 0.5456 (0.83%)\n",
            "Epoch 12/25 | Train: 0.4128 (0.86%) | Test: 0.5175 (0.83%)\n",
            "Epoch 13/25 | Train: 0.3981 (0.86%) | Test: 0.4742 (0.85%)\n",
            "Epoch 14/25 | Train: 0.3831 (0.87%) | Test: 0.5123 (0.84%)\n",
            "Epoch 15/25 | Train: 0.3644 (0.87%) | Test: 0.4602 (0.85%)\n",
            "Epoch 16/25 | Train: 0.3553 (0.88%) | Test: 0.4694 (0.85%)\n",
            "Epoch 17/25 | Train: 0.3423 (0.88%) | Test: 0.4751 (0.85%)\n",
            "Epoch 18/25 | Train: 0.3336 (0.88%) | Test: 0.5118 (0.84%)\n",
            "Epoch 19/25 | Train: 0.3218 (0.89%) | Test: 0.4423 (0.86%)\n",
            "Epoch 20/25 | Train: 0.3114 (0.89%) | Test: 0.4354 (0.86%)\n",
            "Epoch 21/25 | Train: 0.2997 (0.89%) | Test: 0.4845 (0.85%)\n",
            "Epoch 22/25 | Train: 0.2991 (0.90%) | Test: 0.4303 (0.86%)\n",
            "Epoch 23/25 | Train: 0.2864 (0.90%) | Test: 0.4415 (0.86%)\n",
            "Epoch 24/25 | Train: 0.2751 (0.90%) | Test: 0.4097 (0.87%)\n",
            "Epoch 25/25 | Train: 0.2673 (0.91%) | Test: 0.4191 (0.87%)\n",
            "Total time: 622.32 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "EPOCHS = 50\n",
        "\n",
        "# Learning Rate Scheduler (Cosine Annealing)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_one_epoch(train_loader, model, loss_fn, optimizer)\n",
        "    test_loss, test_acc = evaluate(test_loader, model, loss_fn)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | LR: {current_lr:.6f} | \"\n",
        "          f\"Train: {train_loss:.4f} ({train_acc:.2f}%) | \"\n",
        "          f\"Test: {test_loss:.4f} ({test_acc:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m9gUkt8_GEv",
        "outputId": "a36362d9-9d74-4520-c740-60f5e4dcc2e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 | LR: 0.000999 | Train: 1.4486 (0.47%) | Test: 1.1347 (0.59%)\n",
            "Epoch 2/50 | LR: 0.000996 | Train: 1.0178 (0.64%) | Test: 0.8783 (0.69%)\n",
            "Epoch 3/50 | LR: 0.000991 | Train: 0.8319 (0.71%) | Test: 0.7207 (0.75%)\n",
            "Epoch 4/50 | LR: 0.000984 | Train: 0.7277 (0.75%) | Test: 0.6590 (0.77%)\n",
            "Epoch 5/50 | LR: 0.000976 | Train: 0.6633 (0.77%) | Test: 0.6205 (0.79%)\n",
            "Epoch 6/50 | LR: 0.000965 | Train: 0.6129 (0.79%) | Test: 0.6400 (0.79%)\n",
            "Epoch 7/50 | LR: 0.000952 | Train: 0.5777 (0.80%) | Test: 0.5653 (0.81%)\n",
            "Epoch 8/50 | LR: 0.000938 | Train: 0.5488 (0.81%) | Test: 0.5419 (0.82%)\n",
            "Epoch 9/50 | LR: 0.000922 | Train: 0.5174 (0.82%) | Test: 0.5628 (0.81%)\n",
            "Epoch 10/50 | LR: 0.000905 | Train: 0.4938 (0.83%) | Test: 0.4936 (0.84%)\n",
            "Epoch 11/50 | LR: 0.000885 | Train: 0.4744 (0.84%) | Test: 0.4792 (0.84%)\n",
            "Epoch 12/50 | LR: 0.000864 | Train: 0.4532 (0.84%) | Test: 0.4823 (0.85%)\n",
            "Epoch 13/50 | LR: 0.000842 | Train: 0.4405 (0.85%) | Test: 0.4473 (0.85%)\n",
            "Epoch 14/50 | LR: 0.000819 | Train: 0.4176 (0.85%) | Test: 0.4521 (0.85%)\n",
            "Epoch 15/50 | LR: 0.000794 | Train: 0.4090 (0.86%) | Test: 0.4205 (0.86%)\n",
            "Epoch 16/50 | LR: 0.000768 | Train: 0.3882 (0.86%) | Test: 0.4184 (0.86%)\n",
            "Epoch 17/50 | LR: 0.000741 | Train: 0.3747 (0.87%) | Test: 0.4444 (0.85%)\n",
            "Epoch 18/50 | LR: 0.000713 | Train: 0.3663 (0.87%) | Test: 0.4442 (0.86%)\n",
            "Epoch 19/50 | LR: 0.000684 | Train: 0.3532 (0.88%) | Test: 0.4288 (0.86%)\n",
            "Epoch 20/50 | LR: 0.000655 | Train: 0.3394 (0.88%) | Test: 0.4375 (0.86%)\n",
            "Epoch 21/50 | LR: 0.000624 | Train: 0.3309 (0.88%) | Test: 0.3999 (0.87%)\n",
            "Epoch 22/50 | LR: 0.000594 | Train: 0.3141 (0.89%) | Test: 0.4189 (0.87%)\n",
            "Epoch 23/50 | LR: 0.000563 | Train: 0.3073 (0.89%) | Test: 0.3965 (0.87%)\n",
            "Epoch 24/50 | LR: 0.000531 | Train: 0.2947 (0.90%) | Test: 0.3836 (0.88%)\n",
            "Epoch 25/50 | LR: 0.000500 | Train: 0.2855 (0.90%) | Test: 0.4165 (0.87%)\n",
            "Epoch 26/50 | LR: 0.000469 | Train: 0.2770 (0.90%) | Test: 0.3953 (0.88%)\n",
            "Epoch 27/50 | LR: 0.000437 | Train: 0.2659 (0.91%) | Test: 0.3933 (0.88%)\n",
            "Epoch 28/50 | LR: 0.000406 | Train: 0.2586 (0.91%) | Test: 0.3966 (0.88%)\n",
            "Epoch 29/50 | LR: 0.000376 | Train: 0.2531 (0.91%) | Test: 0.3933 (0.88%)\n",
            "Epoch 30/50 | LR: 0.000345 | Train: 0.2395 (0.91%) | Test: 0.3728 (0.88%)\n",
            "Epoch 31/50 | LR: 0.000316 | Train: 0.2295 (0.92%) | Test: 0.3801 (0.89%)\n",
            "Epoch 32/50 | LR: 0.000287 | Train: 0.2235 (0.92%) | Test: 0.3802 (0.88%)\n",
            "Epoch 33/50 | LR: 0.000259 | Train: 0.2177 (0.92%) | Test: 0.3750 (0.89%)\n",
            "Epoch 34/50 | LR: 0.000232 | Train: 0.2121 (0.93%) | Test: 0.3683 (0.89%)\n",
            "Epoch 35/50 | LR: 0.000206 | Train: 0.2034 (0.93%) | Test: 0.3801 (0.89%)\n",
            "Epoch 36/50 | LR: 0.000181 | Train: 0.1969 (0.93%) | Test: 0.3723 (0.89%)\n",
            "Epoch 37/50 | LR: 0.000158 | Train: 0.1938 (0.93%) | Test: 0.3653 (0.89%)\n",
            "Epoch 38/50 | LR: 0.000136 | Train: 0.1847 (0.93%) | Test: 0.3646 (0.89%)\n",
            "Epoch 39/50 | LR: 0.000115 | Train: 0.1783 (0.94%) | Test: 0.3667 (0.89%)\n",
            "Epoch 40/50 | LR: 0.000095 | Train: 0.1766 (0.94%) | Test: 0.3637 (0.89%)\n",
            "Epoch 41/50 | LR: 0.000078 | Train: 0.1692 (0.94%) | Test: 0.3719 (0.90%)\n",
            "Epoch 42/50 | LR: 0.000062 | Train: 0.1662 (0.94%) | Test: 0.3701 (0.89%)\n",
            "Epoch 43/50 | LR: 0.000048 | Train: 0.1646 (0.94%) | Test: 0.3650 (0.90%)\n",
            "Epoch 44/50 | LR: 0.000035 | Train: 0.1629 (0.94%) | Test: 0.3685 (0.89%)\n",
            "Epoch 45/50 | LR: 0.000024 | Train: 0.1573 (0.94%) | Test: 0.3648 (0.90%)\n",
            "Epoch 46/50 | LR: 0.000016 | Train: 0.1585 (0.94%) | Test: 0.3640 (0.90%)\n",
            "Epoch 47/50 | LR: 0.000009 | Train: 0.1545 (0.95%) | Test: 0.3678 (0.89%)\n",
            "Epoch 48/50 | LR: 0.000004 | Train: 0.1534 (0.95%) | Test: 0.3657 (0.90%)\n",
            "Epoch 49/50 | LR: 0.000001 | Train: 0.1565 (0.94%) | Test: 0.3664 (0.90%)\n",
            "Epoch 50/50 | LR: 0.000000 | Train: 0.1534 (0.94%) | Test: 0.3657 (0.90%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "id": "xIrM3KXvFYBR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "137812e5-af5b-4419-dd5d-ce3c02edd89d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu128)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu128)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (1.4.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (26.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.3)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface_hub->timm) (8.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SEBlock(nn.Module):\n",
        "  def __init__(self, in_channels, reduction = 16):\n",
        "    super(SEBlock, self).__init__()\n",
        "\n",
        "    self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
        "    self.excitation = nn.Sequential(\n",
        "        nn.Linear(in_channels, in_channels // reduction, bias  = False),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Linear(in_channels // reduction, in_channels, bias = False),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, c, _, _ = x.size()\n",
        "    y = self.squeeze(x).view(b, c)\n",
        "    y = self.excitation(y).view(b, c, 1, 1)\n",
        "    return x*y\n",
        "\n",
        "# SENet architecture block is like a attention mechanism for CNN which squeeze the incoming layer into important channels and using relu to check for important channes\n",
        "# The Relu will kill unnecessary neurons which doesnt confidence in its predictions and then another layer convert back to its original size with selected important features\n",
        ""
      ],
      "metadata": {
        "id": "jqDRzCXl2xfb"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualSEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualSEBlock, self).__init__()\n",
        "\n",
        "        self.main_path = nn.Sequential(\n",
        "            nn.ReLU(inplace=False),\n",
        "            SeperableConv2d(in_channels, out_channels, stride=stride),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "\n",
        "            nn.ReLU(inplace=True),\n",
        "            SeperableConv2d(out_channels, out_channels, stride=1),\n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.se = SEBlock(out_channels, reduction=16)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.main_path(x)\n",
        "        out = self.se(out)      # Apply Attention\n",
        "        out += self.shortcut(x) # Add Residual\n",
        "        return out\n",
        "\n",
        "# same Xception module as before we added the attention block within residual block"
      ],
      "metadata": {
        "id": "UnUMMB5xI1fW"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniXceptionSE(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(MiniXceptionSE, self).__init__()\n",
        "\n",
        "        # Entry Flow\n",
        "        self.entry = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.block1 = ResidualSEBlock(64, 128, stride=2)\n",
        "        self.block2 = ResidualSEBlock(128, 256, stride=2)\n",
        "        self.block3 = ResidualSEBlock(256, 728, stride=2)\n",
        "\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(0.4) # Slightly lower dropout since SE acts as regularization\n",
        "        self.fc = nn.Linear(728, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.entry(x)\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Wd1nAg8sIw8H"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MiniXceptionSE(num_classes=10).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "EPOCHS = 50\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)"
      ],
      "metadata": {
        "id": "NUrDClTuLvOQ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_one_epoch(train_loader, model, loss_fn, optimizer)\n",
        "    test_loss, test_acc = evaluate(test_loader, model, loss_fn)\n",
        "    scheduler.step()\n",
        "    lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | LR: {lr:.6f} | \"\n",
        "          f\"Train: {train_loss:.4f} ({train_acc:.2f}%) | \"\n",
        "          f\"Test: {test_loss:.4f} ({test_acc:.2f}%)\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"Done in {total_time:.1f}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk6ex33dL35Y",
        "outputId": "01fdfa86-84c7-41f4-a24f-255fac35f89e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 | LR: 0.000999 | Train: 1.5413 (0.43%) | Test: 1.2289 (0.56%)\n",
            "Epoch 2/50 | LR: 0.000996 | Train: 1.1441 (0.59%) | Test: 0.9632 (0.66%)\n",
            "Epoch 3/50 | LR: 0.000991 | Train: 0.9626 (0.66%) | Test: 0.8619 (0.70%)\n",
            "Epoch 4/50 | LR: 0.000984 | Train: 0.8508 (0.70%) | Test: 0.8297 (0.72%)\n",
            "Epoch 5/50 | LR: 0.000976 | Train: 0.7731 (0.73%) | Test: 0.6879 (0.76%)\n",
            "Epoch 6/50 | LR: 0.000965 | Train: 0.7064 (0.75%) | Test: 0.6935 (0.77%)\n",
            "Epoch 7/50 | LR: 0.000952 | Train: 0.6645 (0.77%) | Test: 0.6221 (0.79%)\n",
            "Epoch 8/50 | LR: 0.000938 | Train: 0.6245 (0.78%) | Test: 0.5966 (0.79%)\n",
            "Epoch 9/50 | LR: 0.000922 | Train: 0.5932 (0.79%) | Test: 0.5416 (0.81%)\n",
            "Epoch 10/50 | LR: 0.000905 | Train: 0.5660 (0.80%) | Test: 0.5507 (0.81%)\n",
            "Epoch 11/50 | LR: 0.000885 | Train: 0.5402 (0.81%) | Test: 0.5049 (0.83%)\n",
            "Epoch 12/50 | LR: 0.000864 | Train: 0.5216 (0.82%) | Test: 0.5036 (0.83%)\n",
            "Epoch 13/50 | LR: 0.000842 | Train: 0.5004 (0.83%) | Test: 0.4837 (0.84%)\n",
            "Epoch 14/50 | LR: 0.000819 | Train: 0.4803 (0.83%) | Test: 0.4781 (0.84%)\n",
            "Epoch 15/50 | LR: 0.000794 | Train: 0.4570 (0.84%) | Test: 0.4452 (0.85%)\n",
            "Epoch 16/50 | LR: 0.000768 | Train: 0.4459 (0.84%) | Test: 0.4364 (0.85%)\n",
            "Epoch 17/50 | LR: 0.000741 | Train: 0.4268 (0.85%) | Test: 0.4656 (0.85%)\n",
            "Epoch 18/50 | LR: 0.000713 | Train: 0.4162 (0.85%) | Test: 0.4411 (0.85%)\n",
            "Epoch 19/50 | LR: 0.000684 | Train: 0.3997 (0.86%) | Test: 0.4266 (0.86%)\n",
            "Epoch 20/50 | LR: 0.000655 | Train: 0.3885 (0.86%) | Test: 0.4202 (0.86%)\n",
            "Epoch 21/50 | LR: 0.000624 | Train: 0.3745 (0.87%) | Test: 0.4183 (0.86%)\n",
            "Epoch 22/50 | LR: 0.000594 | Train: 0.3612 (0.87%) | Test: 0.4195 (0.86%)\n",
            "Epoch 23/50 | LR: 0.000563 | Train: 0.3521 (0.88%) | Test: 0.4117 (0.87%)\n",
            "Epoch 24/50 | LR: 0.000531 | Train: 0.3360 (0.88%) | Test: 0.4024 (0.87%)\n",
            "Epoch 25/50 | LR: 0.000500 | Train: 0.3288 (0.88%) | Test: 0.3797 (0.87%)\n",
            "Epoch 26/50 | LR: 0.000469 | Train: 0.3213 (0.89%) | Test: 0.3718 (0.88%)\n",
            "Epoch 27/50 | LR: 0.000437 | Train: 0.3095 (0.89%) | Test: 0.3602 (0.88%)\n",
            "Epoch 28/50 | LR: 0.000406 | Train: 0.2971 (0.90%) | Test: 0.3586 (0.88%)\n",
            "Epoch 29/50 | LR: 0.000376 | Train: 0.2897 (0.90%) | Test: 0.3814 (0.87%)\n",
            "Epoch 30/50 | LR: 0.000345 | Train: 0.2774 (0.90%) | Test: 0.3581 (0.88%)\n",
            "Epoch 31/50 | LR: 0.000316 | Train: 0.2677 (0.91%) | Test: 0.3635 (0.89%)\n",
            "Epoch 32/50 | LR: 0.000287 | Train: 0.2617 (0.91%) | Test: 0.3630 (0.88%)\n",
            "Epoch 33/50 | LR: 0.000259 | Train: 0.2565 (0.91%) | Test: 0.3650 (0.88%)\n",
            "Epoch 34/50 | LR: 0.000232 | Train: 0.2420 (0.91%) | Test: 0.3528 (0.89%)\n",
            "Epoch 35/50 | LR: 0.000206 | Train: 0.2344 (0.92%) | Test: 0.3655 (0.88%)\n",
            "Epoch 36/50 | LR: 0.000181 | Train: 0.2294 (0.92%) | Test: 0.3539 (0.89%)\n",
            "Epoch 37/50 | LR: 0.000158 | Train: 0.2210 (0.92%) | Test: 0.3530 (0.89%)\n",
            "Epoch 38/50 | LR: 0.000136 | Train: 0.2168 (0.92%) | Test: 0.3523 (0.89%)\n",
            "Epoch 39/50 | LR: 0.000115 | Train: 0.2122 (0.92%) | Test: 0.3495 (0.89%)\n",
            "Epoch 40/50 | LR: 0.000095 | Train: 0.2058 (0.93%) | Test: 0.3474 (0.89%)\n",
            "Epoch 41/50 | LR: 0.000078 | Train: 0.1964 (0.93%) | Test: 0.3464 (0.89%)\n",
            "Epoch 42/50 | LR: 0.000062 | Train: 0.2005 (0.93%) | Test: 0.3434 (0.89%)\n",
            "Epoch 43/50 | LR: 0.000048 | Train: 0.1947 (0.93%) | Test: 0.3436 (0.89%)\n",
            "Epoch 44/50 | LR: 0.000035 | Train: 0.1892 (0.93%) | Test: 0.3452 (0.89%)\n",
            "Epoch 45/50 | LR: 0.000024 | Train: 0.1898 (0.93%) | Test: 0.3415 (0.89%)\n",
            "Epoch 46/50 | LR: 0.000016 | Train: 0.1888 (0.93%) | Test: 0.3420 (0.90%)\n",
            "Epoch 47/50 | LR: 0.000009 | Train: 0.1871 (0.93%) | Test: 0.3410 (0.90%)\n",
            "Epoch 48/50 | LR: 0.000004 | Train: 0.1823 (0.94%) | Test: 0.3406 (0.90%)\n",
            "Epoch 49/50 | LR: 0.000001 | Train: 0.1844 (0.93%) | Test: 0.3402 (0.90%)\n",
            "Epoch 50/50 | LR: 0.000000 | Train: 0.1824 (0.93%) | Test: 0.3400 (0.90%)\n",
            "Done in 3563.2s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_VPaFS6TMGCL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}